# Introdução

Dentro do curso de sistemas distribuídos, foram estudadas diversas tecnologias para distribuição de tarefas computacionais: Sockets, MPI, Message-Queue e Web Services. Dentro desses modelos podemos perceber várias formas de se resolver problemas de comunicação, desempenho, ociosidade e facilidade de implementação. O Hadoop resolve de forma elegante e poderosa o problema de _Bag Of Task_ o qual essas outras arquiteturas também tem a possibilidade de resolver.

De uma constante necessidade do Google em manter uma nuvem que precisava lidar com BigData, nasce uma percepção que daria a luz ao Hadoop. Precisamente, é fácil ver que os dois módulos centrais do Hadoop (Sistemas de Arquivo Distribuído e MapReduce) podem ter nascido de problemas diferentes relacionados ao BigData, tal que executando os mesmo são ferramentas poderosas na sua forma independente, juntos, devem compor a base do que há de mais completo em sistemas distribuídos.

No andamento deste relatório, pretende-se explorar questões de configuração, performance, escalabilidade, entre outros requisitos desejaveis para um sistema distribuído usando Hadoop. Almeja-se que ao final deste, seja possível ter uma visão consistente do Hadoop, seus módulos e suas aplicações.
